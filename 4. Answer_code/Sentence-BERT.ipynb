{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "자연어처리는 이번 대회를 통해 처음 접하게 되었습니다.  \n",
    "우선 좋은 대회 열어주셔서 감사합니다!  \n",
    "nlp에 대해 공부할 수 있는 좋은 기회였습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* private score는 11등이지만 제목을 형식에 맞추지 않은 이유는 sentence-bert를 사용해서 임베딩을 했기 때문에, 이것이 pretrained model을 사용해서 cheating을 한 것이 아닌가 하는 생각이 들었기 때문입니다.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://medium.com/genei-technology/richer-sentence-embeddings-using-sentence-bert-part-i-ce1d9e0b1343  \n",
    "처음에는 tfidf를 활용해서 과제를 진행 중이었는데, 임베딩 단계까지 pretrained model을 활용해도 된다는 공지를 보았습니다. 그래서 위의 링크와 같이 bert를 사용해서 sentence embedding을 사용해봐야 겠다는 생각을 했습니다. 하지만 현업에서 text를 다루시는 분들 사이에서도 embedding이라고 생각하는 범위가 천차만별이라 저도 어디까지 임베딩인가 고민을 많이 했습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "그래서 bert를 사용했기 때문에 cheating일 수 있겠다고 생각했습니다. 따라서 이번 코드공유는 앞서 시도했던 tfidf를 이용한 분류와 sentence embedding을 이용한 분류에 대한 시도로 봐주셨으면 감사하겠습니다!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Dense, BatchNormalization, Dropout\n",
    "from tensorflow.keras import Sequential\n",
    "\n",
    "from scipy.sparse import hstack, vstack\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from tqdm import tqdm_notebook as tm\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### tf-tdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'H:/open'\n",
    "train_df = pd.read_csv(path + '/data/train.csv')\n",
    "test_df = pd.read_csv(path + '/data/test_x.csv')\n",
    "sample_submission = pd.read_csv(path + '/data/sample_submission.csv')\n",
    "\n",
    "# 간단한 처리만 해주었습니다.\n",
    "def apply_regular_expression(text):\n",
    "    text = text.lower()\n",
    "    result = re.sub(' +', ' ', text)\n",
    "    return result\n",
    "\n",
    "train_df['preprocessed_text'] = train_df.text.apply(lambda x : apply_regular_expression(x))\n",
    "test_df['preprocessed_text'] = test_df.text.apply(lambda x : apply_regular_expression(x))\n",
    "\n",
    "train_text = train_df.preprocessed_text.tolist()\n",
    "test_text = test_df.preprocessed_text.tolist()\n",
    "label = np.asarray(train_df.author)\n",
    "\n",
    "tfidf = TfidfVectorizer(analyzer='word', sublinear_tf=True, ngram_range = (1, 2),\n",
    "                        max_features=250000, binary=False)\n",
    "\n",
    "tfidf.fit(train_text)\n",
    "\n",
    "# train / validation 따로 나누어 tfidf셋을 만들지 않았습니다.\n",
    "train_tfidf = tfidf.transform(train_text).astype('float32')\n",
    "test_tfidf = tfidf.transform(test_text).astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(128, input_dim=250000, activation='relu'))\n",
    "    model.add(Dropout(0.8))\n",
    "    model.add(Dense(5, activation='softmax'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n",
      "1544/1544 [==============================] - 8s 4ms/step - loss: 1.3589 - accuracy: 0.4468 - val_loss: 0.7406 - val_accuracy: 0.7724\n",
      "Epoch 2/4\n",
      "1544/1544 [==============================] - 6s 4ms/step - loss: 0.6826 - accuracy: 0.7718 - val_loss: 0.5723 - val_accuracy: 0.8059\n",
      "Epoch 3/4\n",
      "1544/1544 [==============================] - 6s 4ms/step - loss: 0.4692 - accuracy: 0.8442 - val_loss: 0.5244 - val_accuracy: 0.8167\n",
      "Epoch 4/4\n",
      "1544/1544 [==============================] - 6s 4ms/step - loss: 0.3507 - accuracy: 0.8846 - val_loss: 0.5124 - val_accuracy: 0.8141\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\programdata\\anaconda3\\envs\\torch\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\sequential.py:430: UserWarning: `model.predict_proba()` is deprecated and will be removed after 2021-01-01. Please use `model.predict()` instead.\n",
      "  warnings.warn('`model.predict_proba()` is deprecated and '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n",
      "1544/1544 [==============================] - 6s 4ms/step - loss: 1.3404 - accuracy: 0.4628 - val_loss: 0.7374 - val_accuracy: 0.7664\n",
      "Epoch 2/4\n",
      "1544/1544 [==============================] - 6s 4ms/step - loss: 0.6605 - accuracy: 0.7790 - val_loss: 0.5801 - val_accuracy: 0.8005\n",
      "Epoch 3/4\n",
      "1544/1544 [==============================] - 6s 4ms/step - loss: 0.4453 - accuracy: 0.8542 - val_loss: 0.5388 - val_accuracy: 0.8061\n",
      "Epoch 4/4\n",
      "1544/1544 [==============================] - 6s 4ms/step - loss: 0.3315 - accuracy: 0.8901 - val_loss: 0.5308 - val_accuracy: 0.8099\n",
      "Epoch 1/4\n",
      "1544/1544 [==============================] - 6s 4ms/step - loss: 1.3476 - accuracy: 0.4542 - val_loss: 0.7277 - val_accuracy: 0.7764\n",
      "Epoch 2/4\n",
      "1544/1544 [==============================] - 5s 4ms/step - loss: 0.6745 - accuracy: 0.7756 - val_loss: 0.5638 - val_accuracy: 0.8038\n",
      "Epoch 3/4\n",
      "1544/1544 [==============================] - 6s 4ms/step - loss: 0.4586 - accuracy: 0.8506 - val_loss: 0.5195 - val_accuracy: 0.8130\n",
      "Epoch 4/4\n",
      "1544/1544 [==============================] - 6s 4ms/step - loss: 0.3467 - accuracy: 0.8849 - val_loss: 0.5072 - val_accuracy: 0.8169\n",
      "Epoch 1/4\n",
      "1544/1544 [==============================] - 6s 4ms/step - loss: 1.3499 - accuracy: 0.4515 - val_loss: 0.7089 - val_accuracy: 0.7808\n",
      "Epoch 2/4\n",
      "1544/1544 [==============================] - 6s 4ms/step - loss: 0.6659 - accuracy: 0.7770 - val_loss: 0.5436 - val_accuracy: 0.8141\n",
      "Epoch 3/4\n",
      "1544/1544 [==============================] - 6s 4ms/step - loss: 0.4561 - accuracy: 0.8484 - val_loss: 0.4990 - val_accuracy: 0.8189\n",
      "Epoch 4/4\n",
      "1544/1544 [==============================] - 6s 4ms/step - loss: 0.3409 - accuracy: 0.8880 - val_loss: 0.4854 - val_accuracy: 0.8245\n",
      "Epoch 1/4\n",
      "1544/1544 [==============================] - 6s 4ms/step - loss: 1.3627 - accuracy: 0.4471 - val_loss: 0.7631 - val_accuracy: 0.7531\n",
      "Epoch 2/4\n",
      "1544/1544 [==============================] - 6s 4ms/step - loss: 0.6836 - accuracy: 0.7715 - val_loss: 0.5969 - val_accuracy: 0.7941\n",
      "Epoch 3/4\n",
      "1544/1544 [==============================] - 6s 4ms/step - loss: 0.4718 - accuracy: 0.8411 - val_loss: 0.5469 - val_accuracy: 0.8021\n",
      "Epoch 4/4\n",
      "1544/1544 [==============================] - 6s 4ms/step - loss: 0.3553 - accuracy: 0.8836 - val_loss: 0.5284 - val_accuracy: 0.8047\n",
      "Epoch 1/4\n",
      "1544/1544 [==============================] - 6s 4ms/step - loss: 1.3554 - accuracy: 0.4551 - val_loss: 0.7345 - val_accuracy: 0.7693\n",
      "Epoch 2/4\n",
      "1544/1544 [==============================] - 6s 4ms/step - loss: 0.6707 - accuracy: 0.7741 - val_loss: 0.5731 - val_accuracy: 0.7976\n",
      "Epoch 3/4\n",
      "1544/1544 [==============================] - 6s 4ms/step - loss: 0.4636 - accuracy: 0.8456 - val_loss: 0.5276 - val_accuracy: 0.8083\n",
      "Epoch 4/4\n",
      "1544/1544 [==============================] - 6s 4ms/step - loss: 0.3421 - accuracy: 0.8887 - val_loss: 0.5185 - val_accuracy: 0.8107\n",
      "Epoch 1/4\n",
      "1544/1544 [==============================] - 6s 4ms/step - loss: 1.3561 - accuracy: 0.4455 - val_loss: 0.7565 - val_accuracy: 0.7560\n",
      "Epoch 2/4\n",
      "1544/1544 [==============================] - 6s 4ms/step - loss: 0.6769 - accuracy: 0.7719 - val_loss: 0.5923 - val_accuracy: 0.7959\n",
      "Epoch 3/4\n",
      "1544/1544 [==============================] - 6s 4ms/step - loss: 0.4632 - accuracy: 0.8483 - val_loss: 0.5469 - val_accuracy: 0.8034\n",
      "Epoch 4/4\n",
      "1544/1544 [==============================] - 6s 4ms/step - loss: 0.3481 - accuracy: 0.8846 - val_loss: 0.5343 - val_accuracy: 0.8098\n",
      "Epoch 1/4\n",
      "1544/1544 [==============================] - 6s 4ms/step - loss: 1.3597 - accuracy: 0.4543 - val_loss: 0.7344 - val_accuracy: 0.7795\n",
      "Epoch 2/4\n",
      "1544/1544 [==============================] - 6s 4ms/step - loss: 0.6826 - accuracy: 0.7684 - val_loss: 0.5710 - val_accuracy: 0.8058\n",
      "Epoch 3/4\n",
      "1544/1544 [==============================] - 6s 4ms/step - loss: 0.4620 - accuracy: 0.8489 - val_loss: 0.5238 - val_accuracy: 0.8105\n",
      "Epoch 4/4\n",
      "1544/1544 [==============================] - 6s 4ms/step - loss: 0.3511 - accuracy: 0.8847 - val_loss: 0.5114 - val_accuracy: 0.8158\n",
      "Epoch 1/4\n",
      "1544/1544 [==============================] - 6s 4ms/step - loss: 1.3343 - accuracy: 0.4641 - val_loss: 0.7030 - val_accuracy: 0.7863\n",
      "Epoch 2/4\n",
      "1544/1544 [==============================] - 6s 4ms/step - loss: 0.6482 - accuracy: 0.7851 - val_loss: 0.5581 - val_accuracy: 0.8099\n",
      "Epoch 3/4\n",
      "1544/1544 [==============================] - 6s 4ms/step - loss: 0.4379 - accuracy: 0.8534 - val_loss: 0.5226 - val_accuracy: 0.8136\n",
      "Epoch 4/4\n",
      "1544/1544 [==============================] - 6s 4ms/step - loss: 0.3292 - accuracy: 0.8920 - val_loss: 0.5160 - val_accuracy: 0.8172\n",
      "Epoch 1/4\n",
      "1544/1544 [==============================] - 6s 4ms/step - loss: 1.3467 - accuracy: 0.4474 - val_loss: 0.7305 - val_accuracy: 0.7773\n",
      "Epoch 2/4\n",
      "1544/1544 [==============================] - 6s 4ms/step - loss: 0.6664 - accuracy: 0.7791 - val_loss: 0.5679 - val_accuracy: 0.8030\n",
      "Epoch 3/4\n",
      "1544/1544 [==============================] - 6s 4ms/step - loss: 0.4644 - accuracy: 0.8445 - val_loss: 0.5246 - val_accuracy: 0.8136\n",
      "Epoch 4/4\n",
      "1544/1544 [==============================] - 6s 4ms/step - loss: 0.3476 - accuracy: 0.8851 - val_loss: 0.5130 - val_accuracy: 0.8180\n"
     ]
    }
   ],
   "source": [
    "df_len = train_tfidf.shape[0]\n",
    "pred = []\n",
    "for i in range(10):\n",
    "    \n",
    "    model = build_model()\n",
    "    \n",
    "    model.compile(loss='sparse_categorical_crossentropy', optimizer=tf.optimizers.Adam(0.001),\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    history = model.fit(x = vstack((train_tfidf[:int(df_len/10*(i))], train_tfidf[int(df_len/10*(i+1)):])),\n",
    "                        y = np.concatenate([label[:int(df_len/10*(i))], label[int(df_len/10*(i+1)):]], axis = 0),\n",
    "                        validation_data = (train_tfidf[int(df_len/10*i):int(df_len/10*(i+1))],\n",
    "                                          label[int(df_len/10*i):int(df_len/10*(i+1))]),\n",
    "                        epochs = 4)\n",
    "    \n",
    "    proba = model.predict_proba(test_tfidf)\n",
    "    \n",
    "    pred.append(proba)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.032128</td>\n",
       "      <td>0.888711</td>\n",
       "      <td>4.978818e-02</td>\n",
       "      <td>0.026257</td>\n",
       "      <td>3.115712e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.120013</td>\n",
       "      <td>0.630619</td>\n",
       "      <td>6.400654e-02</td>\n",
       "      <td>0.042633</td>\n",
       "      <td>1.427283e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.998843</td>\n",
       "      <td>0.000311</td>\n",
       "      <td>2.518804e-04</td>\n",
       "      <td>0.000070</td>\n",
       "      <td>5.242584e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.009940</td>\n",
       "      <td>0.000064</td>\n",
       "      <td>9.558622e-01</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>3.412066e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.947131</td>\n",
       "      <td>0.039883</td>\n",
       "      <td>2.322925e-03</td>\n",
       "      <td>0.008892</td>\n",
       "      <td>1.770776e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19612</th>\n",
       "      <td>0.000040</td>\n",
       "      <td>0.999957</td>\n",
       "      <td>3.958740e-07</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>1.608762e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19613</th>\n",
       "      <td>0.001789</td>\n",
       "      <td>0.000076</td>\n",
       "      <td>7.571139e-04</td>\n",
       "      <td>0.000061</td>\n",
       "      <td>9.973170e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19614</th>\n",
       "      <td>0.007273</td>\n",
       "      <td>0.991528</td>\n",
       "      <td>7.390922e-05</td>\n",
       "      <td>0.001080</td>\n",
       "      <td>4.505838e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19615</th>\n",
       "      <td>0.001368</td>\n",
       "      <td>0.997140</td>\n",
       "      <td>5.903693e-04</td>\n",
       "      <td>0.000745</td>\n",
       "      <td>1.570227e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19616</th>\n",
       "      <td>0.989282</td>\n",
       "      <td>0.000101</td>\n",
       "      <td>3.697864e-03</td>\n",
       "      <td>0.000910</td>\n",
       "      <td>6.008188e-03</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>19617 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              0         1             2         3             4\n",
       "0      0.032128  0.888711  4.978818e-02  0.026257  3.115712e-03\n",
       "1      0.120013  0.630619  6.400654e-02  0.042633  1.427283e-01\n",
       "2      0.998843  0.000311  2.518804e-04  0.000070  5.242584e-04\n",
       "3      0.009940  0.000064  9.558622e-01  0.000014  3.412066e-02\n",
       "4      0.947131  0.039883  2.322925e-03  0.008892  1.770776e-03\n",
       "...         ...       ...           ...       ...           ...\n",
       "19612  0.000040  0.999957  3.958740e-07  0.000002  1.608762e-07\n",
       "19613  0.001789  0.000076  7.571139e-04  0.000061  9.973170e-01\n",
       "19614  0.007273  0.991528  7.390922e-05  0.001080  4.505838e-05\n",
       "19615  0.001368  0.997140  5.903693e-04  0.000745  1.570227e-04\n",
       "19616  0.989282  0.000101  3.697864e-03  0.000910  6.008188e-03\n",
       "\n",
       "[19617 rows x 5 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(np.mean(pred, axis = 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tf-idf와 간단한 layer의 parameter 조정을 통해 다음과 같은 점수를 얻을 수 있었습니다.  \n",
    "**public score : 0.2395313705"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### sentence-embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from sentence_transformers import models, losses\n",
    "from sentence_transformers import SentencesDataset, LoggingHandler, SentenceTransformer, util, InputExample\n",
    "import logging\n",
    "from datetime import datetime\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Dense, BatchNormalization, Dropout\n",
    "from tensorflow.keras import Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.5.0-dev20201123\n",
      "1.8.0.dev20201113\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(format = '%(asctime)s - %(message)s',\n",
    "                   datefmt = \"%Y-%m-%d %H-%M-%S\", level = logging.INFO,\n",
    "                   handlers = [LoggingHandler()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'H:/open/'\n",
    "\n",
    "train_df = pd.read_csv(path+'data/train.csv')\n",
    "test_df = pd.read_csv(path+'data/test_x.csv')\n",
    "submission = pd.read_csv(path+'data/sample_submission.csv')\n",
    "\n",
    "def lower_txt(text):\n",
    "    result = text.lower()\n",
    "    return result\n",
    "\n",
    "train_df['lower_txt'] = train_df['text'].apply(lambda x : lower_txt(x))\n",
    "test_df['lower_txt'] = test_df['text'].apply(lambda x : lower_txt(x))\n",
    "\n",
    "train_label0_index = train_df[train_df['author'] == 0].index\n",
    "train_label1_index = train_df[train_df['author'] == 1].index\n",
    "train_label2_index = train_df[train_df['author'] == 2].index\n",
    "train_label3_index = train_df[train_df['author'] == 3].index\n",
    "train_label4_index = train_df[train_df['author'] == 4].index\n",
    "\n",
    "train_text = train_df['lower_txt']\n",
    "test_text = test_df['lower_txt']\n",
    "label = train_df['author']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "bert-large mean-tokens pooling model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-12-07 14-29-34 - loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-large-uncased-config.json from cache at C:\\Users\\mylov/.cache\\torch\\transformers\\6dfaed860471b03ab5b9acb6153bea82b6632fb9bbe514d3fff050fe1319ee6d.788fed32bb8481a9b15ce726d41c53d5d5066b04c667e34ce3a7a3826d1573d8\n",
      "2020-12-07 14-29-34 - Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "2020-12-07 14-29-36 - loading weights file https://cdn.huggingface.co/bert-large-uncased-pytorch_model.bin from cache at C:\\Users\\mylov/.cache\\torch\\transformers\\73e65a4648c1a5eab31ecea94e04a92a7168cd7089d588b68e5bc057aff40421.4d5343a4b979c4beeaadef17a0453d1bb183dd9b084f58b84c7cc781df343ae6\n",
      "2020-12-07 14-29-42 - All model checkpoint weights were used when initializing BertModel.\n",
      "\n",
      "2020-12-07 14-29-42 - All the weights of BertModel were initialized from the model checkpoint at bert-large-uncased.\n",
      "If your task is similar to the task the model of the ckeckpoint was trained on, you can already use BertModel for predictions without further training.\n",
      "2020-12-07 14-29-43 - loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-large-uncased-config.json from cache at C:\\Users\\mylov/.cache\\torch\\transformers\\6dfaed860471b03ab5b9acb6153bea82b6632fb9bbe514d3fff050fe1319ee6d.788fed32bb8481a9b15ce726d41c53d5d5066b04c667e34ce3a7a3826d1573d8\n",
      "2020-12-07 14-29-43 - Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "2020-12-07 14-29-44 - loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-large-uncased-vocab.txt from cache at C:\\Users\\mylov/.cache\\torch\\transformers\\9b3c03a36e83b13d5ba95ac965c9f9074a99e14340c523ab405703179e79fc46.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
      "2020-12-07 14-29-44 - Use pytorch device: cuda\n"
     ]
    }
   ],
   "source": [
    "model_name = ['bert-large-uncased', 'roberta-large', 'albert-large-v2', 'facebook/bart-large',\n",
    "              \"deepset/bert-large-uncased-whole-word-masking-squad2\", \"deepset/roberta-base-squad2\",\n",
    "              'facebook/bart-large-cnn', \"facebook/bart-large-xsum\"]\n",
    "\n",
    "train_batch_size = 2\n",
    "\n",
    "word_embedding_model = models.Transformer(model_name[0])\n",
    "pooling_model = models.Pooling(word_embedding_model.get_word_embedding_dimension(),\n",
    "                              pooling_mode_mean_tokens = True,\n",
    "                              pooling_mode_cls_token = False,\n",
    "                              pooling_mode_max_tokens = False)\n",
    "\n",
    "bertl_mean_model = SentenceTransformer(modules = [word_embedding_model, pooling_model])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "bert-large max-tokens pooling model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-12-07 14-30-11 - loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-large-uncased-config.json from cache at C:\\Users\\mylov/.cache\\torch\\transformers\\6dfaed860471b03ab5b9acb6153bea82b6632fb9bbe514d3fff050fe1319ee6d.788fed32bb8481a9b15ce726d41c53d5d5066b04c667e34ce3a7a3826d1573d8\n",
      "2020-12-07 14-30-11 - Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "2020-12-07 14-30-11 - loading weights file https://cdn.huggingface.co/bert-large-uncased-pytorch_model.bin from cache at C:\\Users\\mylov/.cache\\torch\\transformers\\73e65a4648c1a5eab31ecea94e04a92a7168cd7089d588b68e5bc057aff40421.4d5343a4b979c4beeaadef17a0453d1bb183dd9b084f58b84c7cc781df343ae6\n",
      "2020-12-07 14-30-16 - All model checkpoint weights were used when initializing BertModel.\n",
      "\n",
      "2020-12-07 14-30-16 - All the weights of BertModel were initialized from the model checkpoint at bert-large-uncased.\n",
      "If your task is similar to the task the model of the ckeckpoint was trained on, you can already use BertModel for predictions without further training.\n",
      "2020-12-07 14-30-16 - loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-large-uncased-config.json from cache at C:\\Users\\mylov/.cache\\torch\\transformers\\6dfaed860471b03ab5b9acb6153bea82b6632fb9bbe514d3fff050fe1319ee6d.788fed32bb8481a9b15ce726d41c53d5d5066b04c667e34ce3a7a3826d1573d8\n",
      "2020-12-07 14-30-16 - Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "2020-12-07 14-30-17 - loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-large-uncased-vocab.txt from cache at C:\\Users\\mylov/.cache\\torch\\transformers\\9b3c03a36e83b13d5ba95ac965c9f9074a99e14340c523ab405703179e79fc46.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
      "2020-12-07 14-30-17 - Use pytorch device: cuda\n"
     ]
    }
   ],
   "source": [
    "train_batch_size = 2\n",
    "\n",
    "word_embedding_model = models.Transformer(model_name[0])\n",
    "pooling_model = models.Pooling(word_embedding_model.get_word_embedding_dimension(),\n",
    "                              pooling_mode_mean_tokens = False,\n",
    "                              pooling_mode_cls_token = False,\n",
    "                              pooling_mode_max_tokens = True)\n",
    "\n",
    "bertl_max_model = SentenceTransformer(modules = [word_embedding_model, pooling_model])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████| 54879/54879 [00:01<00:00, 39247.07it/s]\n"
     ]
    }
   ],
   "source": [
    "label2int = {0 : 0, 1 : 1, 2 : 2, 3 : 3, 4 : 4}\n",
    "\n",
    "np.random.seed(100)\n",
    "train_input = []\n",
    "for i in tqdm(train_df.index):\n",
    "    if label[i] == 0:\n",
    "        sub_text = train_text[np.random.choice(train_label0_index)]\n",
    "    elif label[i] == 1:\n",
    "        sub_text = train_text[np.random.choice(train_label1_index)]\n",
    "    elif label[i] == 2:\n",
    "        sub_text = train_text[np.random.choice(train_label2_index)]\n",
    "    elif label[i] == 3:\n",
    "        sub_text = train_text[np.random.choice(train_label3_index)]\n",
    "    elif label[i] == 4:\n",
    "        sub_text = train_text[np.random.choice(train_label4_index)]\n",
    "    train_input.append(InputExample(texts = [train_text[i], sub_text], label = label2int[label[i]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-12-07 14-32-40 - Make Dataset\n",
      "2020-12-07 14-32-40 - Softmax loss: #Vectors concatenated: 3\n"
     ]
    }
   ],
   "source": [
    "logging.info('Make Dataset')\n",
    "\n",
    "train_dataset = SentencesDataset(train_input, model = bertl_mean_model)\n",
    "train_dataloader = DataLoader(train_dataset, shuffle = True, batch_size = train_batch_size)\n",
    "train_loss = losses.SoftmaxLoss(model = bertl_mean_model, sentence_embedding_dimension = bertl_mean_model.get_sentence_embedding_dimension(),\n",
    "                                num_labels=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 3\n",
    "warmup_steps = math.ceil(len(train_dataset) * num_epochs / train_batch_size * 0.1)\n",
    "logging.info(\"Warmup-steps : {}\".format(warmup_steps))\n",
    "\n",
    "bertl_mean_model.fit(train_objectives = [(train_dataloader, train_loss)],\n",
    "                     epochs = num_epochs,\n",
    "                     warmup_steps=warmup_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-12-07 14-37-58 - Make Dataset\n",
      "2020-12-07 14-37-58 - Softmax loss: #Vectors concatenated: 3\n"
     ]
    }
   ],
   "source": [
    "logging.info('Make Dataset')\n",
    "\n",
    "train_dataset = SentencesDataset(train_input, model = bertl_max_model)\n",
    "train_dataloader = DataLoader(train_dataset, shuffle = True, batch_size = train_batch_size)\n",
    "train_loss = losses.SoftmaxLoss(model = bertl_max_model, sentence_embedding_dimension = bertl_max_model.get_sentence_embedding_dimension(),\n",
    "                                num_labels=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 3\n",
    "warmup_steps = math.ceil(len(train_dataset) * num_epochs / train_batch_size * 0.1)\n",
    "logging.info(\"Warmup-steps : {}\".format(warmup_steps))\n",
    "\n",
    "bertl_max_model.fit(train_objectives = [(train_dataloader, train_loss)],\n",
    "                    epochs = num_epochs,\n",
    "                    warmup_steps=warmup_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.info(\"vector encoding\")\n",
    "\n",
    "train_mean_vec = bertl_mean_model.encode(train_text.tolist())\n",
    "test_mean_vec = bertl_mean_model.encode(test_text.tolist())\n",
    "train_max_vec = bertl_max_model.encode(train_text.tolist())\n",
    "train_mean_vec = bertl_max_model.encode(test_text.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "시간상 csv파일로 저장한 vec값들을 불러오겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''train_max_vec = np.array(pd.read_csv(path + 'data/vector/train_bertl_max3.csv'))\n",
    "test_max_vec = np.array(pd.read_csv(path + 'data/vector/test_bertl_max3.csv'))\n",
    "train_mean_vec = np.array(pd.read_csv(path + 'data/vector/train_bertl_mean3.csv'))\n",
    "test_mean_vec = np.array(pd.read_csv(path + 'data/vector/test_bertl_mean3.csv'))'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mean, max token vector concat\n",
    "train_vec = np.concatenate((train_mean_vec, train_max_vec), axis = 1)\n",
    "test_vec = np.concatenate((test_mean_vec, test_max_vec), axis = 1)\n",
    "label = np.array(train_df['author'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(128, input_dim=train_vec.shape[1], activation='relu'))\n",
    "    model.add(Dropout(0.4))\n",
    "    model.add(Dense(5, activation='softmax'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                           | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/7\n",
      "1544/1544 [==============================] - 5s 3ms/step - loss: 0.2840 - accuracy: 0.9103 - val_loss: 0.1592 - val_accuracy: 0.9453\n",
      "Epoch 2/7\n",
      "1544/1544 [==============================] - 3s 2ms/step - loss: 0.1611 - accuracy: 0.9481 - val_loss: 0.1572 - val_accuracy: 0.9466\n",
      "Epoch 3/7\n",
      "1544/1544 [==============================] - 3s 2ms/step - loss: 0.1565 - accuracy: 0.9498 - val_loss: 0.1535 - val_accuracy: 0.9466\n",
      "Epoch 4/7\n",
      "1544/1544 [==============================] - 3s 2ms/step - loss: 0.1555 - accuracy: 0.9497 - val_loss: 0.1589 - val_accuracy: 0.9471\n",
      "Epoch 5/7\n",
      "1544/1544 [==============================] - 3s 2ms/step - loss: 0.1503 - accuracy: 0.9506 - val_loss: 0.1590 - val_accuracy: 0.9457\n",
      "Epoch 6/7\n",
      "1544/1544 [==============================] - 3s 2ms/step - loss: 0.1444 - accuracy: 0.9547 - val_loss: 0.1532 - val_accuracy: 0.9471\n",
      "Epoch 7/7\n",
      "1544/1544 [==============================] - 3s 2ms/step - loss: 0.1455 - accuracy: 0.9530 - val_loss: 0.1561 - val_accuracy: 0.9451\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\programdata\\anaconda3\\envs\\torch\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\sequential.py:430: UserWarning: `model.predict_proba()` is deprecated and will be removed after 2021-01-01. Please use `model.predict()` instead.\n",
      "  warnings.warn('`model.predict_proba()` is deprecated and '\n",
      " 10%|████████▎                                                                          | 1/10 [00:27<04:04, 27.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/7\n",
      "1544/1544 [==============================] - 4s 2ms/step - loss: 0.2730 - accuracy: 0.9169 - val_loss: 0.1675 - val_accuracy: 0.9481\n",
      "Epoch 2/7\n",
      "1544/1544 [==============================] - 3s 2ms/step - loss: 0.1630 - accuracy: 0.9479 - val_loss: 0.1641 - val_accuracy: 0.9486\n",
      "Epoch 3/7\n",
      "1544/1544 [==============================] - 3s 2ms/step - loss: 0.1562 - accuracy: 0.9501 - val_loss: 0.1625 - val_accuracy: 0.9481\n",
      "Epoch 4/7\n",
      "1544/1544 [==============================] - 3s 2ms/step - loss: 0.1515 - accuracy: 0.9521 - val_loss: 0.1640 - val_accuracy: 0.9484\n",
      "Epoch 5/7\n",
      "1544/1544 [==============================] - 3s 2ms/step - loss: 0.1527 - accuracy: 0.9503 - val_loss: 0.1640 - val_accuracy: 0.9483\n",
      "Epoch 6/7\n",
      "1544/1544 [==============================] - 3s 2ms/step - loss: 0.1457 - accuracy: 0.9517 - val_loss: 0.1661 - val_accuracy: 0.9475\n",
      "Epoch 7/7\n",
      "1544/1544 [==============================] - 3s 2ms/step - loss: 0.1489 - accuracy: 0.9522 - val_loss: 0.1653 - val_accuracy: 0.9486\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 20%|████████████████▌                                                                  | 2/10 [00:51<03:31, 26.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/7\n",
      "1544/1544 [==============================] - 4s 2ms/step - loss: 0.2885 - accuracy: 0.9117 - val_loss: 0.1461 - val_accuracy: 0.9501\n",
      "Epoch 2/7\n",
      "1544/1544 [==============================] - 3s 2ms/step - loss: 0.1600 - accuracy: 0.9494 - val_loss: 0.1416 - val_accuracy: 0.9524\n",
      "Epoch 3/7\n",
      "1544/1544 [==============================] - 3s 2ms/step - loss: 0.1593 - accuracy: 0.9490 - val_loss: 0.1418 - val_accuracy: 0.9526\n",
      "Epoch 4/7\n",
      "1544/1544 [==============================] - 3s 2ms/step - loss: 0.1563 - accuracy: 0.9492 - val_loss: 0.1414 - val_accuracy: 0.9521\n",
      "Epoch 5/7\n",
      "1544/1544 [==============================] - 3s 2ms/step - loss: 0.1539 - accuracy: 0.9489 - val_loss: 0.1412 - val_accuracy: 0.9519\n",
      "Epoch 6/7\n",
      "1544/1544 [==============================] - 3s 2ms/step - loss: 0.1510 - accuracy: 0.9501 - val_loss: 0.1400 - val_accuracy: 0.9524\n",
      "Epoch 7/7\n",
      "1544/1544 [==============================] - 3s 2ms/step - loss: 0.1546 - accuracy: 0.9484 - val_loss: 0.1406 - val_accuracy: 0.9528\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 30%|████████████████████████▉                                                          | 3/10 [01:16<03:00, 25.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/7\n",
      "1544/1544 [==============================] - 4s 2ms/step - loss: 0.3030 - accuracy: 0.9041 - val_loss: 0.1344 - val_accuracy: 0.9537\n",
      "Epoch 2/7\n",
      "1544/1544 [==============================] - 3s 2ms/step - loss: 0.1607 - accuracy: 0.9483 - val_loss: 0.1311 - val_accuracy: 0.9559\n",
      "Epoch 3/7\n",
      "1544/1544 [==============================] - 3s 2ms/step - loss: 0.1569 - accuracy: 0.9488 - val_loss: 0.1319 - val_accuracy: 0.9568\n",
      "Epoch 4/7\n",
      "1544/1544 [==============================] - 3s 2ms/step - loss: 0.1536 - accuracy: 0.9498 - val_loss: 0.1325 - val_accuracy: 0.9557\n",
      "Epoch 5/7\n",
      "1544/1544 [==============================] - 3s 2ms/step - loss: 0.1533 - accuracy: 0.9495 - val_loss: 0.1294 - val_accuracy: 0.9581\n",
      "Epoch 6/7\n",
      "1544/1544 [==============================] - 3s 2ms/step - loss: 0.1525 - accuracy: 0.9497 - val_loss: 0.1287 - val_accuracy: 0.9579\n",
      "Epoch 7/7\n",
      "1544/1544 [==============================] - 3s 2ms/step - loss: 0.1508 - accuracy: 0.9501 - val_loss: 0.1296 - val_accuracy: 0.9574\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 40%|█████████████████████████████████▏                                                 | 4/10 [01:41<02:34, 25.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/7\n",
      "1544/1544 [==============================] - 4s 2ms/step - loss: 0.2832 - accuracy: 0.9121 - val_loss: 0.1539 - val_accuracy: 0.9473\n",
      "Epoch 2/7\n",
      "1544/1544 [==============================] - 3s 2ms/step - loss: 0.1610 - accuracy: 0.9490 - val_loss: 0.1470 - val_accuracy: 0.9506\n",
      "Epoch 3/7\n",
      "1544/1544 [==============================] - 3s 2ms/step - loss: 0.1528 - accuracy: 0.9495 - val_loss: 0.1491 - val_accuracy: 0.9504\n",
      "Epoch 4/7\n",
      "1544/1544 [==============================] - 3s 2ms/step - loss: 0.1526 - accuracy: 0.9499 - val_loss: 0.1474 - val_accuracy: 0.9506\n",
      "Epoch 5/7\n",
      "1544/1544 [==============================] - 3s 2ms/step - loss: 0.1459 - accuracy: 0.9517 - val_loss: 0.1466 - val_accuracy: 0.9515\n",
      "Epoch 6/7\n",
      "1544/1544 [==============================] - 3s 2ms/step - loss: 0.1506 - accuracy: 0.9506 - val_loss: 0.1474 - val_accuracy: 0.9499\n",
      "Epoch 7/7\n",
      "1544/1544 [==============================] - 3s 2ms/step - loss: 0.1513 - accuracy: 0.9500 - val_loss: 0.1470 - val_accuracy: 0.9510\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 50%|█████████████████████████████████████████▌                                         | 5/10 [02:07<02:08, 25.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/7\n",
      "1544/1544 [==============================] - 4s 2ms/step - loss: 0.3113 - accuracy: 0.8993 - val_loss: 0.1614 - val_accuracy: 0.9479\n",
      "Epoch 2/7\n",
      "1544/1544 [==============================] - 3s 2ms/step - loss: 0.1617 - accuracy: 0.9491 - val_loss: 0.1578 - val_accuracy: 0.9493\n",
      "Epoch 3/7\n",
      "1544/1544 [==============================] - 3s 2ms/step - loss: 0.1556 - accuracy: 0.9493 - val_loss: 0.1557 - val_accuracy: 0.9495\n",
      "Epoch 4/7\n",
      "1544/1544 [==============================] - 3s 2ms/step - loss: 0.1538 - accuracy: 0.9502 - val_loss: 0.1556 - val_accuracy: 0.9492\n",
      "Epoch 5/7\n",
      "1544/1544 [==============================] - 3s 2ms/step - loss: 0.1496 - accuracy: 0.9502 - val_loss: 0.1523 - val_accuracy: 0.9503\n",
      "Epoch 6/7\n",
      "1544/1544 [==============================] - 3s 2ms/step - loss: 0.1528 - accuracy: 0.9491 - val_loss: 0.1536 - val_accuracy: 0.9506\n",
      "Epoch 7/7\n",
      "1544/1544 [==============================] - 3s 2ms/step - loss: 0.1490 - accuracy: 0.9510 - val_loss: 0.1504 - val_accuracy: 0.9497\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 60%|█████████████████████████████████████████████████▊                                 | 6/10 [02:32<01:42, 25.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/7\n",
      "1544/1544 [==============================] - 4s 2ms/step - loss: 0.3211 - accuracy: 0.9012 - val_loss: 0.1451 - val_accuracy: 0.9523\n",
      "Epoch 2/7\n",
      "1544/1544 [==============================] - 3s 2ms/step - loss: 0.1651 - accuracy: 0.9471 - val_loss: 0.1442 - val_accuracy: 0.9510\n",
      "Epoch 3/7\n",
      "1544/1544 [==============================] - 3s 2ms/step - loss: 0.1538 - accuracy: 0.9511 - val_loss: 0.1414 - val_accuracy: 0.9515\n",
      "Epoch 4/7\n",
      "1544/1544 [==============================] - 3s 2ms/step - loss: 0.1542 - accuracy: 0.9505 - val_loss: 0.1407 - val_accuracy: 0.9510\n",
      "Epoch 5/7\n",
      "1544/1544 [==============================] - 3s 2ms/step - loss: 0.1514 - accuracy: 0.9498 - val_loss: 0.1406 - val_accuracy: 0.9513\n",
      "Epoch 6/7\n",
      "1544/1544 [==============================] - 3s 2ms/step - loss: 0.1514 - accuracy: 0.9508 - val_loss: 0.1426 - val_accuracy: 0.9512\n",
      "Epoch 7/7\n",
      "1544/1544 [==============================] - 3s 2ms/step - loss: 0.1469 - accuracy: 0.9521 - val_loss: 0.1397 - val_accuracy: 0.9524\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 70%|██████████████████████████████████████████████████████████                         | 7/10 [02:57<01:15, 25.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/7\n",
      "1544/1544 [==============================] - 4s 2ms/step - loss: 0.3175 - accuracy: 0.9029 - val_loss: 0.1508 - val_accuracy: 0.9506\n",
      "Epoch 2/7\n",
      "1544/1544 [==============================] - 3s 2ms/step - loss: 0.1622 - accuracy: 0.9496 - val_loss: 0.1472 - val_accuracy: 0.9519\n",
      "Epoch 3/7\n",
      "1544/1544 [==============================] - 3s 2ms/step - loss: 0.1561 - accuracy: 0.9495 - val_loss: 0.1476 - val_accuracy: 0.9508\n",
      "Epoch 4/7\n",
      "1544/1544 [==============================] - 3s 2ms/step - loss: 0.1579 - accuracy: 0.9478 - val_loss: 0.1459 - val_accuracy: 0.9532\n",
      "Epoch 5/7\n",
      "1544/1544 [==============================] - 3s 2ms/step - loss: 0.1481 - accuracy: 0.9513 - val_loss: 0.1453 - val_accuracy: 0.9532\n",
      "Epoch 6/7\n",
      "1544/1544 [==============================] - 3s 2ms/step - loss: 0.1481 - accuracy: 0.9513 - val_loss: 0.1449 - val_accuracy: 0.9510\n",
      "Epoch 7/7\n",
      "1544/1544 [==============================] - 3s 2ms/step - loss: 0.1474 - accuracy: 0.9516 - val_loss: 0.1447 - val_accuracy: 0.9508\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 80%|██████████████████████████████████████████████████████████████████▍                | 8/10 [03:22<00:50, 25.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/7\n",
      "1544/1544 [==============================] - 4s 2ms/step - loss: 0.3462 - accuracy: 0.8922 - val_loss: 0.1466 - val_accuracy: 0.9541\n",
      "Epoch 2/7\n",
      "1544/1544 [==============================] - 3s 2ms/step - loss: 0.1630 - accuracy: 0.9489 - val_loss: 0.1433 - val_accuracy: 0.9548\n",
      "Epoch 3/7\n",
      "1544/1544 [==============================] - 3s 2ms/step - loss: 0.1592 - accuracy: 0.9489 - val_loss: 0.1433 - val_accuracy: 0.9539\n",
      "Epoch 4/7\n",
      "1544/1544 [==============================] - 3s 2ms/step - loss: 0.1518 - accuracy: 0.9505 - val_loss: 0.1414 - val_accuracy: 0.9552\n",
      "Epoch 5/7\n",
      "1544/1544 [==============================] - 3s 2ms/step - loss: 0.1527 - accuracy: 0.9485 - val_loss: 0.1413 - val_accuracy: 0.9550\n",
      "Epoch 6/7\n",
      "1544/1544 [==============================] - 3s 2ms/step - loss: 0.1497 - accuracy: 0.9511 - val_loss: 0.1426 - val_accuracy: 0.9539\n",
      "Epoch 7/7\n",
      "1544/1544 [==============================] - 3s 2ms/step - loss: 0.1503 - accuracy: 0.9505 - val_loss: 0.1408 - val_accuracy: 0.9534\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 90%|██████████████████████████████████████████████████████████████████████████▋        | 9/10 [03:47<00:25, 25.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/7\n",
      "1544/1544 [==============================] - 4s 2ms/step - loss: 0.2863 - accuracy: 0.9105 - val_loss: 0.1545 - val_accuracy: 0.9492\n",
      "Epoch 2/7\n",
      "1544/1544 [==============================] - 3s 2ms/step - loss: 0.1634 - accuracy: 0.9481 - val_loss: 0.1542 - val_accuracy: 0.9484\n",
      "Epoch 3/7\n",
      "1544/1544 [==============================] - 3s 2ms/step - loss: 0.1547 - accuracy: 0.9504 - val_loss: 0.1539 - val_accuracy: 0.9484\n",
      "Epoch 4/7\n",
      "1544/1544 [==============================] - 3s 2ms/step - loss: 0.1520 - accuracy: 0.9508 - val_loss: 0.1502 - val_accuracy: 0.9495\n",
      "Epoch 5/7\n",
      "1544/1544 [==============================] - 3s 2ms/step - loss: 0.1564 - accuracy: 0.9483 - val_loss: 0.1517 - val_accuracy: 0.9495\n",
      "Epoch 6/7\n",
      "1544/1544 [==============================] - 3s 2ms/step - loss: 0.1526 - accuracy: 0.9508 - val_loss: 0.1519 - val_accuracy: 0.9490\n",
      "Epoch 7/7\n",
      "1544/1544 [==============================] - 3s 2ms/step - loss: 0.1486 - accuracy: 0.9512 - val_loss: 0.1552 - val_accuracy: 0.9503\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 10/10 [04:12<00:00, 25.26s/it]\n"
     ]
    }
   ],
   "source": [
    "length = len(train_vec)\n",
    "result = []\n",
    "\n",
    "for i in tqdm(range(10)):\n",
    "    \n",
    "    dense_model = build_model()\n",
    "\n",
    "    dense_model.compile(loss='sparse_categorical_crossentropy', optimizer=tf.optimizers.Adam(3e-5),\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    dense_model.fit(x = np.concatenate((train_vec[:int(length/10*i)], train_vec[int(length/10*(i+1)):]), axis = 0),\n",
    "                    y = np.concatenate((label[:int(length/10*i)], label[int(length/10*(i+1)):]), axis = 0),\n",
    "                    validation_data = (train_vec[int(length/10*i) : int(length/10*(i+1))],\n",
    "                                       label[int(length/10*i):int(length/10*(i+1))]),\n",
    "                    epochs = 7)\n",
    "    \n",
    "    proba = dense_model.predict_proba(test_vec)\n",
    "    result.append(proba)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       index         0             1             2             3         4\n",
      "0          0  0.000625  1.042035e-01  8.948557e-01  2.511572e-04  0.000064\n",
      "1          1  0.000104  9.998749e-01  1.341021e-06  9.505105e-06  0.000010\n",
      "2          2  0.999997  1.144566e-06  1.151079e-07  3.098989e-07  0.000002\n",
      "3          3  0.000014  1.350951e-03  9.986134e-01  1.225982e-05  0.000009\n",
      "4          4  0.999996  8.629775e-07  1.482152e-07  2.914969e-07  0.000003\n",
      "...      ...       ...           ...           ...           ...       ...\n",
      "19612  19612  0.000020  9.999757e-01  7.097557e-07  1.769100e-06  0.000002\n",
      "19613  19613  0.000002  2.336352e-06  3.140888e-06  6.338286e-07  0.999992\n",
      "19614  19614  0.000016  9.999788e-01  7.378231e-07  1.638135e-06  0.000002\n",
      "19615  19615  0.000016  9.999796e-01  6.562535e-07  1.719821e-06  0.000002\n",
      "19616  19616  0.999745  6.150379e-06  1.322956e-06  3.335471e-05  0.000214\n",
      "\n",
      "[19617 rows x 6 columns]\n"
     ]
    }
   ],
   "source": [
    "# 제출 파일\n",
    "pred = pd.DataFrame(np.mean(result, axis = 0))\n",
    "sub = pd.concat([submission.iloc[:, :1], pred], axis = 1)\n",
    "print(sub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sub.to_csv(path + 'submission/submission.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "public score = 0.1745247836 / private score = 0.17445"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
