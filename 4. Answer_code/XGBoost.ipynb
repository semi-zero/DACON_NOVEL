{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Feature_Engineering_XGBOOST.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q7CSR-GvtNp1"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "\n",
        "train=pd.read_csv('train.csv', encoding='utf-8')\n",
        "test=pd.read_csv('test_x.csv', encoding='utf-8')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sQug2AAKufZG"
      },
      "source": [
        "!pip install textstat\n",
        "!pip install fasttext\n",
        "\n",
        "from textstat import flesch_reading_ease\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "import string\n",
        "import xgboost as xgb\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "from sklearn import metrics, model_selection, naive_bayes\n",
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.tag import pos_tag\n",
        "from nltk import word_tokenize, pos_tag, ne_chunk, tree2conlltags\n",
        "import fasttext\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('punkt')\n",
        "nltk.download('vader_lexicon')\n",
        "nltk.download('maxent_ne_chunker')\n",
        "nltk.download('words')\n",
        "\n",
        "eng_stopwords = set(stopwords.words(\"english\"))\n",
        "symbols_knowns = string.ascii_letters + string.digits + string.punctuation"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IGjoasXsPnds"
      },
      "source": [
        "def sentiment_nltk(text):\n",
        "    res = SentimentIntensityAnalyzer().polarity_scores(text)\n",
        "    return res['compound']\n",
        "\n",
        "def get_words(text):\n",
        "    words = nltk.tokenize.word_tokenize(text)\n",
        "    return [word for word in words if not word in string.punctuation]\n",
        "    \n",
        "def count_tokens(text, tokens):\n",
        "    return sum([w in tokens for w in get_words(text)])\n",
        "\n",
        "def first_word_len(text):\n",
        "    if(len(get_words(text))==0):\n",
        "        return 0\n",
        "    else:   \n",
        "        return len(get_words(text)[0])\n",
        "\n",
        "def last_word_len(text):\n",
        "    if(len(get_words(text))==0):\n",
        "        return 0\n",
        "    else:   \n",
        "        return len(get_words(text)[-1])\n",
        "\n",
        "def symbol_id(x):\n",
        "    symbols=[x for x in symbols_knowns]\n",
        "      \n",
        "    if x not in symbols:\n",
        "        return -1 \n",
        "    else:\n",
        "        return np.where(np.array(symbols) == x )[0][0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WvIH2Wk5QyYI"
      },
      "source": [
        "def fraction_noun(text):\n",
        "    text_splited = text.split(' ')\n",
        "    text_splited = [''.join(c for c in s if c not in string.punctuation) for s in text_splited]\n",
        "    text_splited = [s for s in text_splited if s]\n",
        "    word_count = text_splited.__len__()\n",
        "    if word_count==0:\n",
        "        return 0\n",
        "    else:\n",
        "        pos_list = nltk.pos_tag(text_splited)\n",
        "        noun_count = len([w for w in pos_list if w[1] in ('NN','NNP','NNPS','NNS')])\n",
        "    \n",
        "        return (noun_count/word_count)\n",
        "\n",
        "def fraction_adj(text):\n",
        "    text_splited = text.split(' ')\n",
        "    text_splited = [''.join(c for c in s if c not in string.punctuation) for s in text_splited]\n",
        "    text_splited = [s for s in text_splited if s]\n",
        "    word_count = text_splited.__len__()\n",
        "    if word_count==0:\n",
        "        return 0\n",
        "    else:\n",
        "        pos_list = nltk.pos_tag(text_splited)\n",
        "        adj_count = len([w for w in pos_list if w[1] in ('JJ','JJR','JJS')])\n",
        "    \n",
        "        return (adj_count/word_count)  \n",
        "\n",
        "def fraction_verbs(text):\n",
        "    text_splited = text.split(' ')\n",
        "    text_splited = [''.join(c for c in s if c not in string.punctuation) for s in text_splited]\n",
        "    text_splited = [s for s in text_splited if s]\n",
        "    word_count = text_splited.__len__()\n",
        "    if word_count==0:\n",
        "        return 0\n",
        "    else:\n",
        "        pos_list = nltk.pos_tag(text_splited)\n",
        "        verbs_count = len([w for w in pos_list if w[1] in ('VB','VBD','VBG','VBN','VBP','VBZ')])\n",
        "    \n",
        "        return (verbs_count/word_count)  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zzJsB-PN46Rg"
      },
      "source": [
        "train['num_words']=train['text'].apply(lambda x:len(get_words(x)))\n",
        "train['mean_word_len']=train['text'].apply(lambda x:np.mean([len(w) for w in str(x).split()]))\n",
        "train[\"num_unique_words\"] = train[\"text\"].apply(lambda x: len(set(str(x).split())))\n",
        "train[\"num_chars\"] = train[\"text\"].apply(lambda x: len(str(x)))\n",
        "train[\"num_stopwords\"] = train[\"text\"].apply(lambda x: len([w for w in str(x).lower().split() if w in eng_stopwords]))\n",
        "train[\"num_punctuations\"] =train['text'].apply(lambda x: len([c for c in str(x) if c in string.punctuation]) )\n",
        "train[\"num_words_upper\"] = train[\"text\"].apply(lambda x: len([w for w in str(x).split() if w.isupper()]))/train[\"num_words\"]\n",
        "train[\"num_words_title\"] = train[\"text\"].apply(lambda x: len([w for w in str(x).split() if w.istitle()]))/train[\"num_words\"]\n",
        "train[\"chars_between_comma\"] = train[\"text\"].apply(lambda x: np.mean([len(chunk) for chunk in str(x).split(\",\")]))/train[\"num_chars\"]\n",
        "train[\"symbols_unknowns\"]=train[\"text\"].apply(lambda x: np.sum([not w in symbols_knowns for w in str(x)]))/train[\"num_chars\"]\n",
        "train['noun'] = train[\"text\"].apply(lambda x: fraction_noun(x))\n",
        "train['adj'] = train[\"text\"].apply(lambda x: fraction_adj(x))\n",
        "train['verbs'] = train[\"text\"].apply(lambda x: fraction_verbs(x))\n",
        "train[\"sentiment\"]=train[\"text\"].apply(sentiment_nltk)\n",
        "train['single_frac'] = train['text'].apply(lambda x: count_tokens(x, ['is', 'was', 'has', 'he', 'she', 'it', 'her', 'his']))/train[\"num_words\"]\n",
        "train['plural_frac'] = train['text'].apply(lambda x: count_tokens(x, ['are', 'were', 'have', 'we', 'they']))/train[\"num_words\"]\n",
        "train['first_word_len']=train['text'].apply(first_word_len)/train[\"num_chars\"]\n",
        "train['last_word_len']=train['text'].apply(last_word_len)/train[\"num_chars\"]\n",
        "train[\"first_word_id\"] = train['text'].apply(lambda x: symbol_id(list(x.strip())[0]))\n",
        "train[\"last_word_id\"] = train['text'].apply(lambda x: symbol_id(list(x.strip())[-1]))\n",
        "train['ease']=train['text'].apply(flesch_reading_ease)\n",
        "\n",
        "\n",
        "test['num_words']=test['text'].apply(lambda x:len(str(x).split()))\n",
        "test['mean_word_len']=test['text'].apply(lambda x:np.mean([len(w) for w in str(x).split()]))\n",
        "test[\"num_unique_words\"] = test[\"text\"].apply(lambda x: len(set(str(x).split())))\n",
        "test[\"num_chars\"] = test[\"text\"].apply(lambda x: len(str(x)))\n",
        "test[\"num_stopwords\"] = test[\"text\"].apply(lambda x: len([w for w in str(x).lower().split() if w in eng_stopwords]))\n",
        "test[\"num_punctuations\"] =test['text'].apply(lambda x: len([c for c in str(x) if c in string.punctuation]) )\n",
        "test[\"num_words_upper\"] = test[\"text\"].apply(lambda x: len([w for w in str(x).split() if w.isupper()]))/test[\"num_words\"]\n",
        "test[\"num_words_title\"] = test[\"text\"].apply(lambda x: len([w for w in str(x).split() if w.istitle()]))/test[\"num_words\"]\n",
        "test[\"chars_between_comma\"] = test[\"text\"].apply(lambda x: np.mean([len(chunk) for chunk in str(x).split(\",\")]))/test[\"num_chars\"]\n",
        "test[\"symbols_unknowns\"]=test[\"text\"].apply(lambda x: np.sum([not w in symbols_knowns for w in str(x)]))/test[\"num_chars\"]\n",
        "test['noun'] = test[\"text\"].apply(lambda x: fraction_noun(x))\n",
        "test['adj'] = test[\"text\"].apply(lambda x: fraction_adj(x))\n",
        "test['verbs'] = test[\"text\"].apply(lambda x: fraction_verbs(x))\n",
        "test[\"sentiment\"]=test[\"text\"].apply(sentiment_nltk)\n",
        "test['single_frac'] = test['text'].apply(lambda x: count_tokens(x, ['is', 'was', 'has', 'he', 'she', 'it', 'her', 'his']))/test[\"num_words\"]\n",
        "test['plural_frac'] = test['text'].apply(lambda x: count_tokens(x, ['are', 'were', 'have', 'we', 'they']))/test[\"num_words\"]\n",
        "test['first_word_len']=test['text'].apply(first_word_len)/test[\"num_chars\"]\n",
        "test['last_word_len']=test['text'].apply(last_word_len)/test[\"num_chars\"]\n",
        "test[\"first_word_id\"] = test['text'].apply(lambda x: symbol_id(list(x.strip())[0]))\n",
        "test[\"last_word_id\"] = test['text'].apply(lambda x: symbol_id(list(x.strip())[-1]))\n",
        "test['ease']=test['text'].apply(flesch_reading_ease)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ru7ycu9liOLh"
      },
      "source": [
        "def get_persons(text):\n",
        "    def bind_names(tagged_words):\n",
        "        names=list()\n",
        "        name=list()\n",
        "        for i,w in enumerate(tagged_words):    \n",
        "            if(\"PERSON\" in w[2]):\n",
        "                name.append(w[0])    \n",
        "            else:\n",
        "                if(len(name)!=0):\n",
        "                    names.append(\" \".join(name))\n",
        "                name=list()\n",
        "                \n",
        "            if(i==len(tagged_words)-1 and len(name)!=0):\n",
        "                names.append(\" \".join(name))\n",
        "        return names                   \n",
        "\n",
        "    res_ne_tree = ne_chunk(pos_tag(word_tokenize(text)))\n",
        "    res_ne = tree2conlltags(res_ne_tree)\n",
        "    res_ne_list = [list(x) for x in res_ne]      \n",
        "    return bind_names(res_ne_list)               \n",
        "\n",
        "\n",
        "text_author_0 = \" \".join(list(train['text'][train['author']==0]))\n",
        "text_author_1 = \" \".join(list(train['text'][train['author']==1]))\n",
        "text_author_2 = \" \".join(list(train['text'][train['author']==2]))\n",
        "text_author_3 = \" \".join(list(train['text'][train['author']==3]))\n",
        "text_author_4 = \" \".join(list(train['text'][train['author']==4]))\n",
        "\n",
        "persons_author_0 = set(get_persons(text_author_0))\n",
        "persons_author_1 = set(get_persons(text_author_1))\n",
        "persons_author_2 = set(get_persons(text_author_2))\n",
        "persons_author_3 = set(get_persons(text_author_3))\n",
        "persons_author_4 = set(get_persons(text_author_4))\n",
        "\n",
        "def jaccard(a,b):\n",
        "    return len(a&b)/len(a|b)\n",
        "\n",
        "train[\"persons_0\"]=train[\"text\"].apply(lambda x:jaccard(set(get_persons(x)),persons_author_0)) \n",
        "train[\"persons_1\"]=train[\"text\"].apply(lambda x:jaccard(set(get_persons(x)),persons_author_1)) \n",
        "train[\"persons_2\"]=train[\"text\"].apply(lambda x:jaccard(set(get_persons(x)),persons_author_2)) \n",
        "train[\"persons_3\"]=train[\"text\"].apply(lambda x:jaccard(set(get_persons(x)),persons_author_3)) \n",
        "train[\"persons_4\"]=train[\"text\"].apply(lambda x:jaccard(set(get_persons(x)),persons_author_4)) \n",
        "\n",
        "test[\"persons_0\"]=test[\"text\"].apply(lambda x:jaccard(set(get_persons(x)),persons_author_0)) \n",
        "test[\"persons_1\"]=test[\"text\"].apply(lambda x:jaccard(set(get_persons(x)),persons_author_1)) \n",
        "test[\"persons_2\"]=test[\"text\"].apply(lambda x:jaccard(set(get_persons(x)),persons_author_2)) \n",
        "test[\"persons_3\"]=test[\"text\"].apply(lambda x:jaccard(set(get_persons(x)),persons_author_3)) \n",
        "test[\"persons_4\"]=test[\"text\"].apply(lambda x:jaccard(set(get_persons(x)),persons_author_4)) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yDaj81eKBkka"
      },
      "source": [
        "train['text'].to_csv('sample_file.txt',index=False, header=None, sep=\"\\t\")\n",
        "model_ft = fasttext.train_unsupervised('sample_file.txt', minCount=2, minn=2, maxn=10,dim=300)\n",
        "\n",
        "def sent2vec(s):\n",
        "    words = nltk.tokenize.word_tokenize(s)\n",
        "    #words = [k.stem(w) for w in words]\n",
        "    #words = [w for w in words if not w in string.digits]\n",
        "    #words = [w for w in words if w.isalpha()]\n",
        "    M = []\n",
        "    for w in words:\n",
        "        try:\n",
        "            M.append(model_ft[w])\n",
        "        except:\n",
        "            continue\n",
        "    M = np.array(M)\n",
        "    v = M.sum(axis=0)\n",
        "    if type(v) != np.ndarray:\n",
        "        return np.zeros(300)\n",
        "    return v\n",
        "\n",
        "xtrain_ft = np.array([sent2vec(x) for x in train['text']])\n",
        "xtest_ft = np.array([sent2vec(x) for x in test['text']])\n",
        "\n",
        "train_ft=pd.DataFrame(xtrain_ft)\n",
        "train_ft.columns = ['ft_vector_'+str(i) for i in range(xtrain_ft.shape[1])]\n",
        "\n",
        "test_ft=pd.DataFrame(xtest_ft)\n",
        "test_ft.columns = ['ft_vector_'+str(i) for i in range(xtrain_ft.shape[1])]\n",
        "\n",
        "train = pd.concat([train, train_ft], axis=1)\n",
        "test = pd.concat([test, test_ft], axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Epnmcb8uqk1r"
      },
      "source": [
        "tfidf_vec = TfidfVectorizer(tokenizer=word_tokenize, stop_words=stopwords.words('english'), ngram_range=(1, 3), min_df=50)\n",
        "train_tfidf = tfidf_vec.fit_transform(train['text'].values.tolist())\n",
        "test_tfidf = tfidf_vec.transform(test['text'].values.tolist())\n",
        "train_y = train['author']\n",
        "\n",
        "def runLR(train_X,train_y,test_X,test_y,test_X2):\n",
        "    model=LogisticRegression()\n",
        "    model.fit(train_X,train_y)\n",
        "    pred_test_y=model.predict_proba(test_X)\n",
        "    pred_test_y2=model.predict_proba(test_X2)\n",
        "    return pred_test_y, pred_test_y2, model\n",
        "\n",
        "\n",
        "cv_scores=[]\n",
        "cols_to_drop=['text','index']\n",
        "train_X = train.drop(cols_to_drop+['author'], axis=1)\n",
        "train_y=train['author']\n",
        "test_X = test.drop(cols_to_drop, axis=1)\n",
        "pred_train=np.zeros([train.shape[0],5])\n",
        "pred_full_test = 0\n",
        "\n",
        "cv = model_selection.StratifiedKFold(n_splits=5, shuffle=True, random_state=2020)\n",
        "\n",
        "for dev_index, val_index in cv.split(train_X,train_y):\n",
        "    dev_X, val_X = train_tfidf[dev_index], train_tfidf[val_index]\n",
        "    dev_y, val_y = train_y[dev_index], train_y[val_index]\n",
        "    pred_val_y, pred_test_y, model = runLR(dev_X, dev_y, val_X, val_y,test_tfidf)\n",
        "    pred_full_test = pred_full_test + pred_test_y\n",
        "    pred_train[val_index,:] = pred_val_y\n",
        "    cv_scores.append(metrics.log_loss(val_y, pred_val_y))\n",
        "print(\"Mean cv score : \", np.mean(cv_scores))\n",
        "pred_full_test = pred_full_test / 5.\n",
        "\n",
        "train[\"tfidf_LR_0\"] = pred_train[:,0]\n",
        "train[\"tfidf_LR_1\"] = pred_train[:,1]\n",
        "train[\"tfidf_LR_2\"] = pred_train[:,2]\n",
        "train[\"tfidf_LR_3\"] = pred_train[:,3]\n",
        "train[\"tfidf_LR_4\"] = pred_train[:,4]\n",
        "test[\"tfidf_LR_0\"] = pred_full_test[:,0]\n",
        "test[\"tfidf_LR_1\"] = pred_full_test[:,1]\n",
        "test[\"tfidf_LR_2\"] = pred_full_test[:,2]\n",
        "test[\"tfidf_LR_3\"] = pred_full_test[:,3]\n",
        "test[\"tfidf_LR_4\"] = pred_full_test[:,4]\n",
        "\n",
        "\n",
        "cvec_vec=CountVectorizer(tokenizer=word_tokenize, stop_words=stopwords.words('english'), ngram_range=(1, 3), min_df=50)\n",
        "cvec_vec.fit(train['text'].values.tolist())\n",
        "train_cvec = cvec_vec.transform(train['text'].values.tolist())\n",
        "test_cvec = cvec_vec.transform(test['text'].values.tolist())\n",
        "\n",
        "cv_scores=[]\n",
        "cols_to_drop=['text','index']\n",
        "train_X = train.drop(cols_to_drop+['author'], axis=1)\n",
        "train_y=train['author']\n",
        "test_X = test.drop(cols_to_drop, axis=1)\n",
        "pred_train=np.zeros([train.shape[0],5])\n",
        "pred_full_test = 0\n",
        "\n",
        "cv = model_selection.StratifiedKFold(n_splits=5, shuffle=True, random_state=2020)\n",
        "\n",
        "for dev_index, val_index in cv.split(train_X,train_y):\n",
        "    dev_X, val_X = train_cvec[dev_index], train_cvec[val_index]\n",
        "    dev_y, val_y = train_y[dev_index], train_y[val_index]\n",
        "    pred_val_y, pred_test_y, model = runLR(dev_X, dev_y, val_X, val_y,test_cvec)\n",
        "    pred_full_test = pred_full_test + pred_test_y\n",
        "    pred_train[val_index,:] = pred_val_y\n",
        "    cv_scores.append(metrics.log_loss(val_y, pred_val_y))\n",
        "print(\"Mean cv score : \", np.mean(cv_scores))\n",
        "pred_full_test = pred_full_test / 5.\n",
        "\n",
        "train[\"cvec_LR_0\"] = pred_train[:,0]\n",
        "train[\"cvec_LR_1\"] = pred_train[:,1]\n",
        "train[\"cvec_LR_2\"] = pred_train[:,2]\n",
        "train[\"cvec_LR_3\"] = pred_train[:,3]\n",
        "train[\"cvec_LR_4\"] = pred_train[:,4]\n",
        "test[\"cvec_LR_0\"] = pred_full_test[:,0]\n",
        "test[\"cvec_LR_1\"] = pred_full_test[:,1]\n",
        "test[\"cvec_LR_2\"] = pred_full_test[:,2]\n",
        "test[\"cvec_LR_3\"] = pred_full_test[:,3]\n",
        "test[\"cvec_LR_4\"] = pred_full_test[:,4]\n",
        "\n",
        "\n",
        "cvec_char_vec = CountVectorizer(ngram_range=(1,7), analyzer='char')\n",
        "cvec_char_vec.fit(train['text'].values.tolist())\n",
        "train_cvec_char = cvec_char_vec.transform(train['text'].values.tolist())\n",
        "test_cvec_char = cvec_char_vec.transform(test['text'].values.tolist())\n",
        "\n",
        "cv_scores=[]\n",
        "cols_to_drop=['text','index']\n",
        "train_X = train.drop(cols_to_drop+['author'], axis=1)\n",
        "train_y=train['author']\n",
        "test_X = test.drop(cols_to_drop, axis=1)\n",
        "pred_train=np.zeros([train.shape[0],5])\n",
        "pred_full_test = 0\n",
        "\n",
        "cv = model_selection.StratifiedKFold(n_splits=5, shuffle=True, random_state=2020)\n",
        "\n",
        "for dev_index, val_index in cv.split(train_X,train_y):\n",
        "    dev_X, val_X = train_cvec_char[dev_index], train_cvec_char[val_index]\n",
        "    dev_y, val_y = train_y[dev_index], train_y[val_index]\n",
        "    pred_val_y, pred_test_y, model = runLR(dev_X, dev_y, val_X, val_y,test_cvec_char)\n",
        "    pred_full_test = pred_full_test + pred_test_y\n",
        "    pred_train[val_index,:] = pred_val_y\n",
        "    cv_scores.append(metrics.log_loss(val_y, pred_val_y))\n",
        "print(\"Mean cv score : \", np.mean(cv_scores))\n",
        "pred_full_test = pred_full_test / 5.\n",
        "\n",
        "train[\"cvec_char_LR_0\"] = pred_train[:,0]\n",
        "train[\"cvec_char_LR_1\"] = pred_train[:,1]\n",
        "train[\"cvec_char_LR_2\"] = pred_train[:,2]\n",
        "train[\"cvec_char_LR_3\"] = pred_train[:,3]\n",
        "train[\"cvec_char_LR_4\"] = pred_train[:,4]\n",
        "test[\"cvec_char_LR_0\"] = pred_full_test[:,0]\n",
        "test[\"cvec_char_LR_1\"] = pred_full_test[:,1]\n",
        "test[\"cvec_char_LR_2\"] = pred_full_test[:,2]\n",
        "test[\"cvec_char_LR_3\"] = pred_full_test[:,3]\n",
        "test[\"cvec_char_LR_4\"] = pred_full_test[:,4]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PjoWMNZ-rTD0"
      },
      "source": [
        "tfidf_vec = TfidfVectorizer(tokenizer=word_tokenize, stop_words=stopwords.words('english'), ngram_range=(1, 3), min_df=50)\n",
        "\n",
        "train_tfidf = tfidf_vec.fit_transform(train['text'].values.tolist())\n",
        "test_tfidf = tfidf_vec.transform(test['text'].values.tolist())\n",
        "train_y = train['author']\n",
        "\n",
        "def runSGD(train_X,train_y,test_X,test_y,test_X2):\n",
        "    model=SGDClassifier(loss='log')\n",
        "    model.fit(train_X,train_y)\n",
        "    pred_test_y=model.predict_proba(test_X)\n",
        "    pred_test_y2=model.predict_proba(test_X2)\n",
        "    return pred_test_y, pred_test_y2, model\n",
        "\n",
        "cv_scores=[]\n",
        "cols_to_drop=['text','index']\n",
        "train_X = train.drop(cols_to_drop+['author'], axis=1)\n",
        "train_y=train['author']\n",
        "test_X = test.drop(cols_to_drop, axis=1)\n",
        "pred_train=np.zeros([train.shape[0],5])\n",
        "pred_full_test = 0\n",
        "\n",
        "cv = model_selection.StratifiedKFold(n_splits=5, shuffle=True, random_state=2020)\n",
        "\n",
        "for dev_index, val_index in cv.split(train_X,train_y):\n",
        "    dev_X, val_X = train_tfidf[dev_index], train_tfidf[val_index]\n",
        "    dev_y, val_y = train_y[dev_index], train_y[val_index]\n",
        "    pred_val_y, pred_test_y, model = runSGD(dev_X, dev_y, val_X, val_y,test_tfidf)\n",
        "    pred_full_test = pred_full_test + pred_test_y\n",
        "    pred_train[val_index,:] = pred_val_y\n",
        "    cv_scores.append(metrics.log_loss(val_y, pred_val_y))\n",
        "print(\"Mean cv score : \", np.mean(cv_scores))\n",
        "pred_full_test = pred_full_test / 5.\n",
        "\n",
        "train[\"tfidf_SGD_0\"] = pred_train[:,0]\n",
        "train[\"tfidf_SGD_1\"] = pred_train[:,1]\n",
        "train[\"tfidf_SGD_2\"] = pred_train[:,2]\n",
        "train[\"tfidf_SGD_3\"] = pred_train[:,3]\n",
        "train[\"tfidf_SGD_4\"] = pred_train[:,4]\n",
        "test[\"tfidf_SGD_0\"] = pred_full_test[:,0]\n",
        "test[\"tfidf_SGD_1\"] = pred_full_test[:,1]\n",
        "test[\"tfidf_SGD_2\"] = pred_full_test[:,2]\n",
        "test[\"tfidf_SGD_3\"] = pred_full_test[:,3]\n",
        "test[\"tfidf_SGD_4\"] = pred_full_test[:,4]\n",
        "\n",
        "\n",
        "cvec_char_vec = CountVectorizer(ngram_range=(1,7), analyzer='char')\n",
        "cvec_char_vec.fit(train['text'].values.tolist())\n",
        "train_cvec_char = cvec_char_vec.transform(train['text'].values.tolist())\n",
        "test_cvec_char = cvec_char_vec.transform(test['text'].values.tolist())\n",
        "\n",
        "cv_scores=[]\n",
        "cols_to_drop=['text','index']\n",
        "train_X = train.drop(cols_to_drop+['author'], axis=1)\n",
        "train_y=train['author']\n",
        "test_X = test.drop(cols_to_drop, axis=1)\n",
        "pred_train=np.zeros([train.shape[0],5])\n",
        "pred_full_test = 0\n",
        "\n",
        "cv = model_selection.StratifiedKFold(n_splits=5, shuffle=True, random_state=2020)\n",
        "\n",
        "for dev_index, val_index in cv.split(train_X,train_y):\n",
        "    dev_X, val_X = train_cvec_char[dev_index], train_cvec_char[val_index]\n",
        "    dev_y, val_y = train_y[dev_index], train_y[val_index]\n",
        "    pred_val_y, pred_test_y, model = runSGD(dev_X, dev_y, val_X, val_y,test_cvec_char)\n",
        "    pred_full_test = pred_full_test + pred_test_y\n",
        "    pred_train[val_index,:] = pred_val_y\n",
        "    cv_scores.append(metrics.log_loss(val_y, pred_val_y))\n",
        "print(\"Mean cv score : \", np.mean(cv_scores))\n",
        "pred_full_test = pred_full_test / 5.\n",
        "\n",
        "train[\"cvec_char_SGD_0\"] = pred_train[:,0]\n",
        "train[\"cvec_char_SGD_1\"] = pred_train[:,1]\n",
        "train[\"cvec_char_SGD_2\"] = pred_train[:,2]\n",
        "train[\"cvec_char_SGD_3\"] = pred_train[:,3]\n",
        "train[\"cvec_char_SGD_4\"] = pred_train[:,4]\n",
        "test[\"cvec_char_SGD_0\"] = pred_full_test[:,0]\n",
        "test[\"cvec_char_SGD_1\"] = pred_full_test[:,1]\n",
        "test[\"cvec_char_SGD_2\"] = pred_full_test[:,2]\n",
        "test[\"cvec_char_SGD_3\"] = pred_full_test[:,3]\n",
        "test[\"cvec_char_SGD_4\"] = pred_full_test[:,4]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BGERaoMorpfs"
      },
      "source": [
        "tfidf_vec = TfidfVectorizer(tokenizer=word_tokenize, stop_words=stopwords.words('english'), ngram_range=(1, 3), min_df=50)\n",
        "\n",
        "train_tfidf = tfidf_vec.fit_transform(train['text'].values.tolist())\n",
        "test_tfidf = tfidf_vec.transform(test['text'].values.tolist())\n",
        "train_y = train['author']\n",
        "\n",
        "def runRF(train_X,train_y,test_X,test_y,test_X2):\n",
        "    model=RandomForestClassifier()\n",
        "    model.fit(train_X,train_y)\n",
        "    pred_test_y=model.predict_proba(test_X)\n",
        "    pred_test_y2=model.predict_proba(test_X2)\n",
        "    return pred_test_y, pred_test_y2, model\n",
        "\n",
        "\n",
        "cv_scores=[]\n",
        "cols_to_drop=['text','index']\n",
        "train_X = train.drop(cols_to_drop+['author'], axis=1)\n",
        "train_y=train['author']\n",
        "test_X = test.drop(cols_to_drop, axis=1)\n",
        "pred_train=np.zeros([train.shape[0],5])\n",
        "pred_full_test = 0\n",
        "\n",
        "cv = model_selection.StratifiedKFold(n_splits=5, shuffle=True, random_state=2020)\n",
        "\n",
        "for dev_index, val_index in cv.split(train_X,train_y):\n",
        "    dev_X, val_X = train_tfidf[dev_index], train_tfidf[val_index]\n",
        "    dev_y, val_y = train_y[dev_index], train_y[val_index]\n",
        "    pred_val_y, pred_test_y, model = runRF(dev_X, dev_y, val_X, val_y,test_tfidf)\n",
        "    pred_full_test = pred_full_test + pred_test_y\n",
        "    pred_train[val_index,:] = pred_val_y\n",
        "    cv_scores.append(metrics.log_loss(val_y, pred_val_y))\n",
        "print(\"Mean cv score : \", np.mean(cv_scores))\n",
        "pred_full_test = pred_full_test / 5.\n",
        "\n",
        "train[\"tfidf_RF_0\"] = pred_train[:,0]\n",
        "train[\"tfidf_RF_1\"] = pred_train[:,1]\n",
        "train[\"tfidf_RF_2\"] = pred_train[:,2]\n",
        "train[\"tfidf_RF_3\"] = pred_train[:,3]\n",
        "train[\"tfidf_RF_4\"] = pred_train[:,4]\n",
        "test[\"tfidf_RF_0\"] = pred_full_test[:,0]\n",
        "test[\"tfidf_RF_1\"] = pred_full_test[:,1]\n",
        "test[\"tfidf_RF_2\"] = pred_full_test[:,2]\n",
        "test[\"tfidf_RF_3\"] = pred_full_test[:,3]\n",
        "test[\"tfidf_RF_4\"] = pred_full_test[:,4]\n",
        "\n",
        "\n",
        "cvec_char_vec = CountVectorizer(ngram_range=(1,7), analyzer='char')\n",
        "cvec_char_vec.fit(train['text'].values.tolist())\n",
        "train_cvec_char = cvec_char_vec.transform(train['text'].values.tolist())\n",
        "test_cvec_char = cvec_char_vec.transform(test['text'].values.tolist())\n",
        "\n",
        "cv_scores=[]\n",
        "cols_to_drop=['text','index']\n",
        "train_X = train.drop(cols_to_drop+['author'], axis=1)\n",
        "train_y=train['author']\n",
        "test_X = test.drop(cols_to_drop, axis=1)\n",
        "pred_train=np.zeros([train.shape[0],5])\n",
        "pred_full_test = 0\n",
        "\n",
        "cv = model_selection.StratifiedKFold(n_splits=5, shuffle=True, random_state=2020)\n",
        "\n",
        "for dev_index, val_index in cv.split(train_X,train_y):\n",
        "    dev_X, val_X = train_cvec_char[dev_index], train_cvec_char[val_index]\n",
        "    dev_y, val_y = train_y[dev_index], train_y[val_index]\n",
        "    pred_val_y, pred_test_y, model = runRF(dev_X, dev_y, val_X, val_y,test_cvec_char)\n",
        "    pred_full_test = pred_full_test + pred_test_y\n",
        "    pred_train[val_index,:] = pred_val_y\n",
        "    cv_scores.append(metrics.log_loss(val_y, pred_val_y))\n",
        "print(\"Mean cv score : \", np.mean(cv_scores))\n",
        "pred_full_test = pred_full_test / 5.\n",
        "\n",
        "train[\"cvec_char_RF_0\"] = pred_train[:,0]\n",
        "train[\"cvec_char_RF_1\"] = pred_train[:,1]\n",
        "train[\"cvec_char_RF_2\"] = pred_train[:,2]\n",
        "train[\"cvec_char_RF_3\"] = pred_train[:,3]\n",
        "train[\"cvec_char_RF_4\"] = pred_train[:,4]\n",
        "test[\"cvec_char_RF_0\"] = pred_full_test[:,0]\n",
        "test[\"cvec_char_RF_1\"] = pred_full_test[:,1]\n",
        "test[\"cvec_char_RF_2\"] = pred_full_test[:,2]\n",
        "test[\"cvec_char_RF_3\"] = pred_full_test[:,3]\n",
        "test[\"cvec_char_RF_4\"] = pred_full_test[:,4]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fdMOm7bt7zMB"
      },
      "source": [
        "tfidf_vec = TfidfVectorizer(tokenizer=word_tokenize, stop_words=stopwords.words('english'), ngram_range=(1, 3), min_df=50)\n",
        "\n",
        "train_tfidf = tfidf_vec.fit_transform(train['text'].values.tolist())\n",
        "test_tfidf = tfidf_vec.transform(test['text'].values.tolist())\n",
        "train_y = train['author']\n",
        "\n",
        "def runMLP(train_X,train_y,test_X,test_y,test_X2):\n",
        "    model=MLPClassifier()\n",
        "    model.fit(train_X,train_y)\n",
        "    pred_test_y=model.predict_proba(test_X)\n",
        "    pred_test_y2=model.predict_proba(test_X2)\n",
        "    return pred_test_y, pred_test_y2, model\n",
        "\n",
        "\n",
        "cv_scores=[]\n",
        "cols_to_drop=['text','index']\n",
        "train_X = train.drop(cols_to_drop+['author'], axis=1)\n",
        "train_y=train['author']\n",
        "test_X = test.drop(cols_to_drop, axis=1)\n",
        "pred_train=np.zeros([train.shape[0],5])\n",
        "pred_full_test = 0\n",
        "\n",
        "cv = model_selection.StratifiedKFold(n_splits=5, shuffle=True, random_state=2020)\n",
        "\n",
        "for dev_index, val_index in cv.split(train_X,train_y):\n",
        "    dev_X, val_X = train_tfidf[dev_index], train_tfidf[val_index]\n",
        "    dev_y, val_y = train_y[dev_index], train_y[val_index]\n",
        "    pred_val_y, pred_test_y, model = runMLP(dev_X, dev_y, val_X, val_y,test_tfidf)\n",
        "    pred_full_test = pred_full_test + pred_test_y\n",
        "    pred_train[val_index,:] = pred_val_y\n",
        "    cv_scores.append(metrics.log_loss(val_y, pred_val_y))\n",
        "print(\"Mean cv score : \", np.mean(cv_scores))\n",
        "pred_full_test = pred_full_test / 5.\n",
        "\n",
        "train[\"tfidf_MLP_0\"] = pred_train[:,0]\n",
        "train[\"tfidf_MLP_1\"] = pred_train[:,1]\n",
        "train[\"tfidf_MLP_2\"] = pred_train[:,2]\n",
        "train[\"tfidf_MLP_3\"] = pred_train[:,3]\n",
        "train[\"tfidf_MLP_4\"] = pred_train[:,4]\n",
        "test[\"tfidf_MLP_0\"] = pred_full_test[:,0]\n",
        "test[\"tfidf_MLP_1\"] = pred_full_test[:,1]\n",
        "test[\"tfidf_MLP_2\"] = pred_full_test[:,2]\n",
        "test[\"tfidf_MLP_3\"] = pred_full_test[:,3]\n",
        "test[\"tfidf_MLP_4\"] = pred_full_test[:,4]\n",
        "\n",
        "\n",
        "cvec_char_vec = CountVectorizer(ngram_range=(1,5), analyzer='char')\n",
        "cvec_char_vec.fit(train['text'].values.tolist())\n",
        "train_cvec_char = cvec_char_vec.transform(train['text'].values.tolist())\n",
        "test_cvec_char = cvec_char_vec.transform(test['text'].values.tolist())\n",
        "train_y = train['author']\n",
        "\n",
        "cv_scores=[]\n",
        "cols_to_drop=['text','index']\n",
        "train_X = train.drop(cols_to_drop+['author'], axis=1)\n",
        "train_y=train['author']\n",
        "test_X = test.drop(cols_to_drop, axis=1)\n",
        "pred_train=np.zeros([train.shape[0],5])\n",
        "pred_full_test = 0\n",
        "\n",
        "cv = model_selection.StratifiedKFold(n_splits=5, shuffle=True, random_state=2020)\n",
        "kf = model_selection.KFold(n_splits=5, shuffle=True, random_state=2020)\n",
        "\n",
        "for dev_index, val_index in cv.split(train_X,train_y):\n",
        "    dev_X, val_X = train_cvec_char[dev_index], train_cvec_char[val_index]\n",
        "    dev_y, val_y = train_y[dev_index], train_y[val_index]\n",
        "    pred_val_y, pred_test_y, model = runMLP(dev_X, dev_y, val_X, val_y,test_cvec_char)\n",
        "    pred_full_test = pred_full_test + pred_test_y\n",
        "    pred_train[val_index,:] = pred_val_y\n",
        "    cv_scores.append(metrics.log_loss(val_y, pred_val_y))\n",
        "print(\"Mean cv score : \", np.mean(cv_scores))\n",
        "pred_full_test = pred_full_test / 5.\n",
        "\n",
        "train[\"cvec_char_MLP_0\"] = pred_train[:,0]\n",
        "train[\"cvec_char_MLP_1\"] = pred_train[:,1]\n",
        "train[\"cvec_char_MLP_2\"] = pred_train[:,2]\n",
        "train[\"cvec_char_MLP_3\"] = pred_train[:,3]\n",
        "train[\"cvec_char_MLP_4\"] = pred_train[:,4]\n",
        "test[\"cvec_char_MLP_0\"] = pred_full_test[:,0]\n",
        "test[\"cvec_char_MLP_1\"] = pred_full_test[:,1]\n",
        "test[\"cvec_char_MLP_2\"] = pred_full_test[:,2]\n",
        "test[\"cvec_char_MLP_3\"] = pred_full_test[:,3]\n",
        "test[\"cvec_char_MLP_4\"] = pred_full_test[:,4]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GA7HEDUNsUPc"
      },
      "source": [
        "tfidf_vec = TfidfVectorizer(tokenizer=word_tokenize, stop_words=stopwords.words('english'), ngram_range=(1, 3), min_df=50)\n",
        "train_tfidf = tfidf_vec.fit_transform(train['text'].values.tolist())\n",
        "test_tfidf = tfidf_vec.transform(test['text'].values.tolist())\n",
        "train_y = train['author']\n",
        "\n",
        "def runDT(train_X,train_y,test_X,test_y,test_X2):\n",
        "    model=DecisionTreeClassifier()\n",
        "    model.fit(train_X,train_y)\n",
        "    pred_test_y=model.predict_proba(test_X)\n",
        "    pred_test_y2=model.predict_proba(test_X2)\n",
        "    return pred_test_y, pred_test_y2, model\n",
        "\n",
        "cv_scores=[]\n",
        "cols_to_drop=['text','index']\n",
        "train_X = train.drop(cols_to_drop+['author'], axis=1)\n",
        "train_y=train['author']\n",
        "test_X = test.drop(cols_to_drop, axis=1)\n",
        "pred_train=np.zeros([train.shape[0],5])\n",
        "pred_full_test = 0\n",
        "\n",
        "cv = model_selection.StratifiedKFold(n_splits=5, shuffle=True, random_state=2020)\n",
        "\n",
        "\n",
        "for dev_index, val_index in cv.split(train_X,train_y):\n",
        "    dev_X, val_X = train_tfidf[dev_index], train_tfidf[val_index]\n",
        "    dev_y, val_y = train_y[dev_index], train_y[val_index]\n",
        "    pred_val_y, pred_test_y, model = runDT(dev_X, dev_y, val_X, val_y,test_tfidf)\n",
        "    pred_full_test = pred_full_test + pred_test_y\n",
        "    pred_train[val_index,:] = pred_val_y\n",
        "    cv_scores.append(metrics.log_loss(val_y, pred_val_y))\n",
        "print(\"Mean cv score : \", np.mean(cv_scores))\n",
        "pred_full_test = pred_full_test / 5.\n",
        "\n",
        "train[\"tfidf_DT_0\"] = pred_train[:,0]\n",
        "train[\"tfidf_DT_1\"] = pred_train[:,1]\n",
        "train[\"tfidf_DT_2\"] = pred_train[:,2]\n",
        "train[\"tfidf_DT_3\"] = pred_train[:,3]\n",
        "train[\"tfidf_DT_4\"] = pred_train[:,4]\n",
        "test[\"tfidf_DT_0\"] = pred_full_test[:,0]\n",
        "test[\"tfidf_DT_1\"] = pred_full_test[:,1]\n",
        "test[\"tfidf_DT_2\"] = pred_full_test[:,2]\n",
        "test[\"tfidf_DT_3\"] = pred_full_test[:,3]\n",
        "test[\"tfidf_DT_4\"] = pred_full_test[:,4]\n",
        "\n",
        "\n",
        "cvec_char_vec = CountVectorizer(ngram_range=(1,7), analyzer='char')\n",
        "cvec_char_vec.fit(train['text'].values.tolist())\n",
        "train_cvec_char = cvec_char_vec.transform(train['text'].values.tolist())\n",
        "test_cvec_char = cvec_char_vec.transform(test['text'].values.tolist())\n",
        "train_y = train['author']\n",
        "\n",
        "cv_scores=[]\n",
        "cols_to_drop=['text','index']\n",
        "train_X = train.drop(cols_to_drop+['author'], axis=1)\n",
        "train_y=train['author']\n",
        "test_X = test.drop(cols_to_drop, axis=1)\n",
        "pred_train=np.zeros([train.shape[0],5])\n",
        "pred_full_test = 0\n",
        "\n",
        "cv = model_selection.StratifiedKFold(n_splits=5, shuffle=True, random_state=2020)\n",
        "kf = model_selection.KFold(n_splits=5, shuffle=True, random_state=2020)\n",
        "\n",
        "for dev_index, val_index in cv.split(train_X,train_y):\n",
        "    dev_X, val_X = train_cvec_char[dev_index], train_cvec_char[val_index]\n",
        "    dev_y, val_y = train_y[dev_index], train_y[val_index]\n",
        "    pred_val_y, pred_test_y, model = runDT(dev_X, dev_y, val_X, val_y,test_cvec_char)\n",
        "    pred_full_test = pred_full_test + pred_test_y\n",
        "    pred_train[val_index,:] = pred_val_y\n",
        "    cv_scores.append(metrics.log_loss(val_y, pred_val_y))\n",
        "print(\"Mean cv score : \", np.mean(cv_scores))\n",
        "pred_full_test = pred_full_test / 5.\n",
        "\n",
        "train[\"cvec_char_DT_0\"] = pred_train[:,0]\n",
        "train[\"cvec_char_DT_1\"] = pred_train[:,1]\n",
        "train[\"cvec_char_DT_2\"] = pred_train[:,2]\n",
        "train[\"cvec_char_DT_3\"] = pred_train[:,3]\n",
        "train[\"cvec_char_DT_4\"] = pred_train[:,4]\n",
        "test[\"cvec_char_DT_0\"] = pred_full_test[:,0]\n",
        "test[\"cvec_char_DT_1\"] = pred_full_test[:,1]\n",
        "test[\"cvec_char_DT_2\"] = pred_full_test[:,2]\n",
        "test[\"cvec_char_DT_3\"] = pred_full_test[:,3]\n",
        "test[\"cvec_char_DT_4\"] = pred_full_test[:,4]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NNXYbcYRpcaO"
      },
      "source": [
        "tfidf_vec=TfidfVectorizer(stop_words='english',ngram_range=(1,3))\n",
        "train_tfidf= tfidf_vec.fit_transform(train['text'].values.tolist())\n",
        "test_tfidf = tfidf_vec.transform(test['text'].values.tolist())\n",
        "\n",
        "n_comp = 20\n",
        "svd_obj = TruncatedSVD(n_components=n_comp, algorithm='arpack')\n",
        "svd_obj.fit(train_tfidf)\n",
        "\n",
        "train_svd = svd_obj.transform(train_tfidf)\n",
        "test_svd = svd_obj.transform(test_tfidf)\n",
        "\n",
        "from sklearn import preprocessing\n",
        "scl = preprocessing.StandardScaler()\n",
        "scl.fit(train_svd)\n",
        "train_svd_scl = pd.DataFrame(scl.transform(train_svd))\n",
        "test_svd_scl = pd.DataFrame(scl.transform(test_svd))\n",
        "\n",
        "train_svd_scl.columns = ['svd_word_'+str(i) for i in range(n_comp)]\n",
        "test_svd_scl.columns = ['svd_word_'+str(i) for i in range(n_comp)]\n",
        "train = pd.concat([train, train_svd_scl], axis=1)\n",
        "test = pd.concat([test, test_svd_scl], axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ucr-w-IIIidI"
      },
      "source": [
        "def runMNB(train_X,train_y,test_X,test_y,test_X2):\n",
        "    model=naive_bayes.MultinomialNB()\n",
        "    model.fit(train_X,train_y)\n",
        "    pred_test_y=model.predict_proba(test_X)\n",
        "    pred_test_y2=model.predict_proba(test_X2)\n",
        "    return pred_test_y, pred_test_y2, model\n",
        "\n",
        "Count_vec=CountVectorizer(stop_words='english',ngram_range=(1,3))\n",
        "\n",
        "Count_vec.fit(train['text'].values.tolist())\n",
        "train_Count = Count_vec.transform(train['text'].values.tolist())\n",
        "test_Count = Count_vec.transform(test['text'].values.tolist())\n",
        "\n",
        "cv_scores=[]\n",
        "pred_train=np.zeros([train.shape[0],5])\n",
        "pred_full_test = 0\n",
        "\n",
        "kf = model_selection.KFold(n_splits=5, shuffle=True, random_state=2020)\n",
        "\n",
        "for dev_index, val_index in kf.split(train_X):\n",
        "    dev_X, val_X = train_Count[dev_index], train_Count[val_index]\n",
        "    dev_y, val_y = train_y[dev_index], train_y[val_index]\n",
        "    pred_val_y, pred_test_y, model = runMNB(dev_X, dev_y, val_X, val_y,test_Count)\n",
        "    pred_full_test = pred_full_test + pred_test_y\n",
        "    pred_train[val_index,:] = pred_val_y\n",
        "    cv_scores.append(metrics.log_loss(val_y, pred_val_y))\n",
        "print(\"Mean cv score : \", np.mean(cv_scores))\n",
        "pred_full_test = pred_full_test / 5.\n",
        "\n",
        "train[\"nb_cvec_0\"] = pred_train[:,0]\n",
        "train[\"nb_cvec_1\"] = pred_train[:,1]\n",
        "train[\"nb_cvec_2\"] = pred_train[:,2]\n",
        "train[\"nb_cvec_3\"] = pred_train[:,3]\n",
        "train[\"nb_cvec_4\"] = pred_train[:,4]\n",
        "test[\"nb_cvec_0\"] = pred_full_test[:,0]\n",
        "test[\"nb_cvec_1\"] = pred_full_test[:,1]\n",
        "test[\"nb_cvec_2\"] = pred_full_test[:,2]\n",
        "test[\"nb_cvec_3\"] = pred_full_test[:,3]\n",
        "test[\"nb_cvec_4\"] = pred_full_test[:,4]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K4ltGexsOmkZ"
      },
      "source": [
        "cvec_char_vec = CountVectorizer(ngram_range=(1,7), analyzer='char')\n",
        "cvec_char_vec.fit(train['text'].values.tolist())\n",
        "train_cvec_char = cvec_char_vec.transform(train['text'].values.tolist())\n",
        "test_cvec_char = cvec_char_vec.transform(test['text'].values.tolist())\n",
        "\n",
        "cv_scores = []\n",
        "pred_full_test = 0\n",
        "pred_train = np.zeros([train.shape[0], 5])\n",
        "kf = model_selection.KFold(n_splits=5, shuffle=True, random_state=2020)\n",
        "for dev_index, val_index in kf.split(train_X):\n",
        "    dev_X, val_X = train_cvec_char[dev_index], train_cvec_char[val_index]\n",
        "    dev_y, val_y = train_y[dev_index], train_y[val_index]\n",
        "    pred_val_y, pred_test_y, model = runMNB(dev_X, dev_y, val_X, val_y, test_cvec_char)\n",
        "    pred_full_test = pred_full_test + pred_test_y\n",
        "    pred_train[val_index,:] = pred_val_y\n",
        "    cv_scores.append(metrics.log_loss(val_y, pred_val_y))\n",
        "print(\"Mean cv score : \", np.mean(cv_scores))\n",
        "pred_full_test = pred_full_test / 5.\n",
        "\n",
        "train[\"nb_cvec_char_0\"] = pred_train[:,0]\n",
        "train[\"nb_cvec_char_1\"] = pred_train[:,1]\n",
        "train[\"nb_cvec_char_2\"] = pred_train[:,2]\n",
        "train[\"nb_cvec_char_3\"] = pred_train[:,3]\n",
        "train[\"nb_cvec_char_4\"] = pred_train[:,4]\n",
        "test[\"nb_cvec_char_0\"] = pred_full_test[:,0]\n",
        "test[\"nb_cvec_char_1\"] = pred_full_test[:,1]\n",
        "test[\"nb_cvec_char_2\"] = pred_full_test[:,2]\n",
        "test[\"nb_cvec_char_3\"] = pred_full_test[:,3]\n",
        "test[\"nb_cvec_char_4\"] = pred_full_test[:,4]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mqnLKmbrRo2M"
      },
      "source": [
        "tfidf_vec = TfidfVectorizer(ngram_range=(1,5), analyzer='char')\n",
        "\n",
        "train_tfidf = tfidf_vec.fit_transform(train['text'].values.tolist())\n",
        "test_tfidf = tfidf_vec.transform(test['text'].values.tolist())\n",
        "\n",
        "cv_scores = []\n",
        "pred_full_test = 0\n",
        "pred_train = np.zeros([train.shape[0], 5])\n",
        "kf = model_selection.KFold(n_splits=5, shuffle=True, random_state=2020)\n",
        "for dev_index, val_index in kf.split(train_X):\n",
        "    dev_X, val_X = train_tfidf[dev_index], train_tfidf[val_index]\n",
        "    dev_y, val_y = train_y[dev_index], train_y[val_index]\n",
        "    pred_val_y, pred_test_y, model = runMNB(dev_X, dev_y, val_X, val_y, test_tfidf)\n",
        "    pred_full_test = pred_full_test + pred_test_y\n",
        "    pred_train[val_index,:] = pred_val_y\n",
        "    cv_scores.append(metrics.log_loss(val_y, pred_val_y))\n",
        "print(\"Mean cv score : \", np.mean(cv_scores))\n",
        "pred_full_test = pred_full_test / 5.\n",
        "\n",
        "\n",
        "train[\"nb_tfidf_char_0\"] = pred_train[:,0]\n",
        "train[\"nb_tfidf_char_1\"] = pred_train[:,1]\n",
        "train[\"nb_tfidf_char_2\"] = pred_train[:,2]\n",
        "train[\"nb_tfidf_char_3\"] = pred_train[:,3]\n",
        "train[\"nb_tfidf_char_4\"] = pred_train[:,4]\n",
        "test[\"nb_tfidf_char_0\"] = pred_full_test[:,0]\n",
        "test[\"nb_tfidf_char_1\"] = pred_full_test[:,1]\n",
        "test[\"nb_tfidf_char_2\"] = pred_full_test[:,2]\n",
        "test[\"nb_tfidf_char_3\"] = pred_full_test[:,3]\n",
        "test[\"nb_tfidf_char_4\"] = pred_full_test[:,4]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ll1FcETPUKjT"
      },
      "source": [
        "n_comp = 20\n",
        "svd_obj = TruncatedSVD(n_components=n_comp, algorithm='arpack')\n",
        "svd_obj.fit(train_tfidf)\n",
        "\n",
        "train_svd = svd_obj.transform(train_tfidf)\n",
        "test_svd = svd_obj.transform(test_tfidf)\n",
        "\n",
        "from sklearn import preprocessing\n",
        "scl = preprocessing.StandardScaler()\n",
        "scl.fit(train_svd)\n",
        "train_svd_scl = pd.DataFrame(scl.transform(train_svd))\n",
        "test_svd_scl = pd.DataFrame(scl.transform(test_svd))\n",
        "\n",
        "train_svd_scl.columns = ['svd_char_'+str(i) for i in range(n_comp)]\n",
        "test_svd_scl.columns = ['svd_char_'+str(i) for i in range(n_comp)]\n",
        "train = pd.concat([train, train_svd_scl], axis=1)\n",
        "test = pd.concat([test, test_svd_scl], axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8n8Q3RoEVlYU"
      },
      "source": [
        "cols_to_drop = ['index', 'text']\n",
        "train_X = train.drop(cols_to_drop+['author'], axis=1)\n",
        "train_y=train['author']\n",
        "test_index = test['index'].values\n",
        "test_X = test.drop(cols_to_drop, axis=1)\n",
        "xgb_preds=[]\n",
        "kf = model_selection.KFold(n_splits=5, shuffle=True, random_state=2020)\n",
        "\n",
        "for dev_index, val_index in kf.split(train_X):\n",
        "    dev_X, val_X = train_X.loc[dev_index], train_X.loc[val_index]\n",
        "    dev_y, val_y = train_y[dev_index], train_y[val_index]\n",
        "    \n",
        "    dtrain = xgb.DMatrix(dev_X,label=dev_y)\n",
        "    dvalid = xgb.DMatrix(val_X, label=val_y)\n",
        "    watchlist = [(dtrain, 'train'), (dvalid, 'valid')]\n",
        "\n",
        "    param = {}\n",
        "    param['objective'] = 'multi:softprob'\n",
        "    param['eta'] = 0.1\n",
        "    param['max_depth'] = 3\n",
        "    param['silent'] = 1\n",
        "    param['num_class'] = 5\n",
        "    param['eval_metric'] = \"mlogloss\"\n",
        "    param['min_child_weight'] = 1\n",
        "    param['subsample'] = 0.8\n",
        "    param['colsample_bytree'] = 0.3\n",
        "    param['seed'] = 0\n",
        "    param['tree_method'] = 'gpu_hist'\n",
        "\n",
        "    model = xgb.train(param, dtrain, 2000, watchlist, early_stopping_rounds=50, verbose_eval=20)\n",
        "\n",
        "    xgtest2 = xgb.DMatrix(test_X)\n",
        "    xgb_pred = model.predict(xgtest2, ntree_limit = model.best_ntree_limit)\n",
        "    xgb_preds.append(list(xgb_pred))\n",
        "\n",
        "#out_df = pd.DataFrame(pred_full_test)\n",
        "#out_df.columns = ['0','1','2','3','4']\n",
        "#out_df.insert(0, 'index', test_index)\n",
        "#out_df.to_csv(\"submission.csv\", index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z5DlrX81sEEb"
      },
      "source": [
        "fig, ax = plt.subplots(figsize=(12,12))\n",
        "xgb.plot_importance(model, max_num_features=80, height=0.8, ax=ax)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "61ofdv8LoBZU"
      },
      "source": [
        "for i in range(len(xgb_preds[0])):\n",
        "    sum=0\n",
        "    for j in range(5):\n",
        "        sum+=xgb_preds[j][i]    \n",
        "    if(i==0):\n",
        "        preds=sum/5\n",
        "    else:\n",
        "        preds=np.vstack([preds,sum/5])\n",
        "\n",
        "preds=pd.DataFrame(preds)\n",
        "\n",
        "preds.to_csv('submission.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kDZbiWXsGBlf"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}